import pandas as pd
import numpy as np
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt

#Load data

df = pd.read_csv('/Users/WaleedAlzarooni/Desktop/winequality-red.csv')

file_path = '/Users/WaleedAlzarooni/Desktop/winequality-red.csv'

df = pd.read_csv(file_path, delimiter=';')

from sklearn.model_selection import train_test_split

#Seperate relavent columns for X,Y

X = df.drop(columns='quality')  
y = df['quality']               

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train.shape, X_test.shape, y_train.shape, y_test.shape

# Setup training and test sets
X_train_full = X_train.values  # All features for training data
y_train_full = y_train.values  # Target variable (quality) for training data

X_test_full = X_test.values  # All features for test data
y_test_full = y_test.values  # Target variable (quality) for test data

# Add bias term
X_train_full_with_bias = np.c_[np.ones(X_train_full.shape[0]), X_train_full]
X_test_full_with_bias = np.c_[np.ones(X_test_full.shape[0]), X_test_full]

w = (X^T X)^-1 X^T y
# Compute the weights using the normal equation
weights_full = np.linalg.inv(X_train_full_with_bias.T.dot(X_train_full_with_bias)).dot(X_train_full_with_bias.T).dot(y_train_full)

# Make predictions on the training set
y_train_pred_full = X_train_full_with_bias.dot(weights_full)

# Compute the sum-of-squares error on the training set
sse_full = np.sum((y_train_full - y_train_pred_full) ** 2)
print(f"Sum-of-squares error on training set: {sse_full}")

#RMSE
train_rmse = np.sqrt(mean_squared_error(y_train_full, y_train_pred_full))
print(f"Root Mean Squared Error (RMSE) on Training Set: {train_rmse}")

y_test_pred_full = X_test_full_with_bias.dot(weights_full)

test_rmse = np.sqrt(mean_squared_error(y_test_full, y_test_pred_full))
print(f"Root Mean Squared Error (RMSE) on Test Set: {test_rmse}")

# Plot actual vs predicted values
plt.figure(figsize=(8, 6))
plt.scatter(y_train_full, y_train_pred_full, alpha=0.7, edgecolor='k')
plt.plot([y_train_full.min(), y_train_full.max()], [y_train_full.min(), y_train_full.max()], color='red', linewidth=2, linestyle='--')
plt.xlabel("Actual Target Values")
plt.ylabel("Predicted Target Values")
plt.title("Actual vs Predicted Target Values (Training Data)")
plt.grid(True)
plt.show()
print("we can see here that the training data generalises reasonably well to a linear model")

# Hyperparameters
learning_rate = 0.00001  # Small learning rate to ensure convergence
n_iterations = 100    # Number of iterations

# Randomly initialize weights
weights_lms = np.random.randn(X_train_full_with_bias.shape[1])

# LMS algorithm for updating weights
for iteration in range(n_iterations):
    # Make predictions on the training set
    y_train_pred_lms = X_train_full_with_bias.dot(weights_lms)
    
    # Compute the error between predicted and actual values
    error = y_train_full - y_train_pred_lms
    
    # Update the weights (LMS update rule)
    weights_lms += learning_rate * X_train_full_with_bias.T.dot(error) / len(X_train_full)
    
    # Optionally, compute RMSE at each iteration to monitor convergence
    

# Final weights after LMS updates
print(f"Final weights after LMS: {weights_lms}")

# Make predictions on the test set
y_test_pred_lms = X_test_full_with_bias.dot(weights_lms)

train_rmse_lms = np.sqrt(mean_squared_error(y_train_full, y_train_pred_lms))
print(f"Root Mean Squared Error (RMSE) on Train Set: {train_rmse_lms}")

# Compute RMSE for the test set
test_rmse_lms = np.sqrt(mean_squared_error(y_test_full, y_test_pred_lms))
print(f"Root Mean Squared Error (RMSE) on Test Set: {test_rmse_lms}")


